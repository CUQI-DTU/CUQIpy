
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "user/_auto_tutorials/multiple_likelihoods.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_user__auto_tutorials_multiple_likelihoods.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_user__auto_tutorials_multiple_likelihoods.py:


Setting a Bayesian model with multiple likelihoods 
==================================================

In this example we build a PDE-based Bayesian inverse problem where the Bayesian
model has multiple likelihood functions (two different likelihood functions in
this case, but it can be readily extended to more functions) for the same 
Bayesian parameter `theta`, which represents the conductivity parameters in a
1D Poisson problem. Each likelihood is associated with a different model set
up. The models we use here are obtained from the test problem 
:class:`cuqi.testproblem.Poisson1D`. See the class
:class:`cuqi.testproblem.Poisson1D` documentation for more details about the
forward model.

.. GENERATED FROM PYTHON SOURCE LINES 16-17

First we import the python libraries needed.

.. GENERATED FROM PYTHON SOURCE LINES 17-22

.. code-block:: Python

    import cuqi
    import numpy as np
    import matplotlib.pyplot as plt
    from math import ceil








.. GENERATED FROM PYTHON SOURCE LINES 23-31

Choose one of the two cases we study in this demo 
-------------------------------------------------

We can choose between two cases:
Choose `set_up = set_ups[0]` for the case where we have two 1D Poisson models
that differ in the observation operator only. And choose `set_up = set_ups[1]`
for the case where we have two 1D Poisson models that differ in the source 
term only. Here we demonstrate the first case, `set_up = set_ups[0]`.

.. GENERATED FROM PYTHON SOURCE LINES 31-38

.. code-block:: Python


    set_ups = ["multi_observation", "multi_source"]
    set_up = set_ups[0]

    assert set_up == "multi_observation" or set_up == "multi_source",\
        "set_up must be either 'multi_observation' or 'multi_source'"








.. GENERATED FROM PYTHON SOURCE LINES 39-41

Set up the parameters used in both models
-----------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 41-56

.. code-block:: Python


    dim = 50  # Number of the model grid points
    endpoint = 1  # The model domain is the interval [0, endpoint]
    field_type = "Step"  # The conductivity (or diffusivity) field type.
                         # We choose step function parameterization here.
    SNR = 400  # Signal-to-noise ratio
    n_steps = 2  # Number of steps in the conductivity (or diffusivity) step field
    magnitude = 100 # Magnitude of the source term in the Poisson problem

    # Exact solution
    x_exact = np.empty(dim)
    x_exact[:ceil(dim/2)] = 2
    x_exact[ceil(dim/2):] = 3









.. GENERATED FROM PYTHON SOURCE LINES 57-64

Set up the first model
----------------------

We set up the first forward model to have observations at the first half of 
the domain (or observation everywhere if `set_up = set_ups[1]`). We then plot
the true conductivity field (the exact solution), the exact data and the noisy
data.

.. GENERATED FROM PYTHON SOURCE LINES 64-90

.. code-block:: Python


    observation_grid_map1 = None
    if set_up == "multi_observation":
    	# Observe on the first half of the domain
    	observation_grid_map1 = lambda x: x[np.where(x<.5)] 

    # The source term signal
    source1 = lambda xs: magnitude*np.sin(xs*2*np.pi/endpoint)+magnitude

    # Obtain the forward model from the test problem
    model1, data1, problemInfo1 = cuqi.testproblem.Poisson1D(dim=dim,
        endpoint=endpoint,
        field_type=field_type,
        field_params={"n_steps": n_steps},
        observation_grid_map=observation_grid_map1,
        exactSolution=x_exact,
        source=source1,
        SNR=SNR).get_components()

    # Plot data, exact data and exact solution
    plt.figure()
    data1.plot(label='data')
    problemInfo1.exactData.plot(label='exact data')
    problemInfo1.exactSolution.plot(label='exact solution')
    plt.legend()




.. image-sg:: /user/_auto_tutorials/images/sphx_glr_multiple_likelihoods_001.png
   :alt: multiple likelihoods
   :srcset: /user/_auto_tutorials/images/sphx_glr_multiple_likelihoods_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f58a2f6a6c0>



.. GENERATED FROM PYTHON SOURCE LINES 91-98

Set up the second model
-----------------------

We set up the second forward model to have observations at the second half of
the domain (or observation everywhere and and different source term if
`set_up = set_ups[1]`). We then plot the true conductivity field (the exact
solution), the exact data and the noisy data.

.. GENERATED FROM PYTHON SOURCE LINES 98-127

.. code-block:: Python


    observation_grid_map2 = None
    if set_up == "multi_observation":
            # Observe on the second half of the domain
    	observation_grid_map2 = lambda x: x[np.where(x>=.5)]

    # The source term signal
    if set_up == "multi_source":
    	source2 = lambda xs: magnitude*np.sin(2*xs*2*np.pi/endpoint)+magnitude
    else:
    	source2 = source1

    # Obtain the forward model from the test problem
    model2, data2, problemInfo2 = cuqi.testproblem.Poisson1D(dim=dim,
        endpoint=endpoint,
        field_type=field_type,
        field_params={"n_steps": n_steps},
        observation_grid_map=observation_grid_map2,
        exactSolution=x_exact,
        source=source2,
        SNR=SNR).get_components()

    # Plot data, exact data and exact solution
    plt.figure()
    data2.plot(label='data2')
    problemInfo2.exactData.plot(label='exact data2')
    problemInfo2.exactSolution.plot(label='exact solution2')
    plt.legend()




.. image-sg:: /user/_auto_tutorials/images/sphx_glr_multiple_likelihoods_002.png
   :alt: multiple likelihoods
   :srcset: /user/_auto_tutorials/images/sphx_glr_multiple_likelihoods_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f58a2dcef30>



.. GENERATED FROM PYTHON SOURCE LINES 128-134

Create the prior
----------------

Create the prior for the Bayesian parameter `theta`, which is the expansion 
coefficients of the conductivity (or diffusivity) step function. We use a 
Gaussian prior.

.. GENERATED FROM PYTHON SOURCE LINES 134-142

.. code-block:: Python


    theta = cuqi.distribution.Gaussian(
        mean=np.zeros(model1.domain_dim),
        cov=3,
        geometry=model1.domain_geometry,
    )









.. GENERATED FROM PYTHON SOURCE LINES 143-145

Create the data distributions using the two forward models
----------------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 145-163

.. code-block:: Python


    # Estimate the data noise standard deviation
    sigma_noise1 = np.linalg.norm(problemInfo1.exactData)/SNR
    sigma_noise2 = np.linalg.norm(problemInfo2.exactData)/SNR

    # Create the data distributions
    y1 = cuqi.distribution.Gaussian(
        mean=model1(theta),
        cov=sigma_noise1**2,
        geometry=model1.range_geometry,
    )
    y2 = cuqi.distribution.Gaussian(
        mean=model2(theta),
        cov=sigma_noise2**2,
        geometry=model2.range_geometry,
    )









.. GENERATED FROM PYTHON SOURCE LINES 164-167

Formulate the Bayesian inverse problem using the first data distribution (single likelihood)
----------------------------------------------------------------------------------------------------------
We first formulate the Bayesian inverse problem using the first data distribution and analyze the posterior samples.

.. GENERATED FROM PYTHON SOURCE LINES 169-171

Create the posterior 
~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 171-174

.. code-block:: Python


    z1 = cuqi.distribution.JointDistribution(theta,y1)(y1=data1)








.. GENERATED FROM PYTHON SOURCE LINES 175-176

We print the joint distribution `z1`:

.. GENERATED FROM PYTHON SOURCE LINES 176-178

.. code-block:: Python

    print(z1)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Posterior(
        Equation:
             p(theta|y1) ‚àù L(theta|y1)p(theta)
        Densities:
            y1 ~ CUQI Gaussian Likelihood function. Parameters ['theta'].
            theta ~ CUQI Gaussian.
     )




.. GENERATED FROM PYTHON SOURCE LINES 179-183

We see that we obtain a :class:`cuqi.distribution.Posterior` object, which
represents the posterior distribution of the parameters `theta` given the data
`y1`. The posterior distribution in this case is proportional to the product
of the likelihood obtained from the first data distribution and the prior.

.. GENERATED FROM PYTHON SOURCE LINES 185-187

Sample from the posterior
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 187-196

.. code-block:: Python


    # Sample from the posterior
    sampler = cuqi.legacy.sampler.MH(z1)
    samples = sampler.sample_adapt(8000)

    # Plot the credible interval and compute the ESS
    samples.burnthin(1000).plot_ci(95, exact=problemInfo1.exactSolution)
    samples.compute_ess()




.. image-sg:: /user/_auto_tutorials/images/sphx_glr_multiple_likelihoods_003.png
   :alt: multiple likelihoods
   :srcset: /user/_auto_tutorials/images/sphx_glr_multiple_likelihoods_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/CUQIpy/CUQIpy/cuqi/legacy/sampler/_sampler.py:156: UserWarning: 
    You are using the legacy sampler 'MH'.
    This will be removed in a future release of CUQIpy.
    Please consider using the new samplers in the 'cuqi.sampler' module.

      super().__init__(target, x0=x0, dim=dim, **kwargs)
    Sample 80 / 8000    Sample 160 / 8000    Sample 240 / 8000    Sample 320 / 8000    Sample 400 / 8000    Sample 480 / 8000    Sample 560 / 8000    Sample 640 / 8000    Sample 720 / 8000    Sample 800 / 8000    Sample 880 / 8000    Sample 960 / 8000    Sample 1040 / 8000    Sample 1120 / 8000    Sample 1200 / 8000    Sample 1280 / 8000    Sample 1360 / 8000    Sample 1440 / 8000    Sample 1520 / 8000    Sample 1600 / 8000    Sample 1680 / 8000    Sample 1760 / 8000    Sample 1840 / 8000    Sample 1920 / 8000    Sample 2000 / 8000    Sample 2080 / 8000    Sample 2160 / 8000    Sample 2240 / 8000    Sample 2320 / 8000    Sample 2400 / 8000    Sample 2480 / 8000    Sample 2560 / 8000    Sample 2640 / 8000    Sample 2720 / 8000    Sample 2800 / 8000    Sample 2880 / 8000    Sample 2960 / 8000    Sample 3040 / 8000    Sample 3120 / 8000    Sample 3200 / 8000    Sample 3280 / 8000    Sample 3360 / 8000    Sample 3440 / 8000    Sample 3520 / 8000    Sample 3600 / 8000    Sample 3680 / 8000    Sample 3760 / 8000    Sample 3840 / 8000    Sample 3920 / 8000    Sample 4000 / 8000    Sample 4080 / 8000    Sample 4160 / 8000    Sample 4240 / 8000    Sample 4320 / 8000    Sample 4400 / 8000    Sample 4480 / 8000    Sample 4560 / 8000    Sample 4640 / 8000    Sample 4720 / 8000    Sample 4800 / 8000    Sample 4880 / 8000    Sample 4960 / 8000    Sample 5040 / 8000    Sample 5120 / 8000    Sample 5200 / 8000    Sample 5280 / 8000    Sample 5360 / 8000    Sample 5440 / 8000    Sample 5520 / 8000    Sample 5600 / 8000    Sample 5680 / 8000    Sample 5760 / 8000    Sample 5840 / 8000    Sample 5920 / 8000    Sample 6000 / 8000    Sample 6080 / 8000    Sample 6160 / 8000    Sample 6240 / 8000    Sample 6320 / 8000    Sample 6400 / 8000    Sample 6480 / 8000    Sample 6560 / 8000    Sample 6640 / 8000    Sample 6720 / 8000    Sample 6800 / 8000    Sample 6880 / 8000    Sample 6960 / 8000    Sample 7040 / 8000    Sample 7120 / 8000    Sample 7200 / 8000    Sample 7280 / 8000    Sample 7360 / 8000    Sample 7440 / 8000    Sample 7520 / 8000    Sample 7600 / 8000    Sample 7680 / 8000    Sample 7760 / 8000    Sample 7840 / 8000    Sample 7920 / 8000    Sample 8000 / 8000    Sample 8000 / 8000

    Average acceptance rate: 0.108375 MCMC scale: 0.05370189213198375 


    array([172.13470253, 109.82438384])



.. GENERATED FROM PYTHON SOURCE LINES 197-200

Formulate the Bayesian inverse problem using the second data distribution (single likelihood)
------------------------------------------------------------------------------------------------------------
We then formulate the Bayesian inverse problem using the second data distribution and analyze the posterior samples.

.. GENERATED FROM PYTHON SOURCE LINES 202-204

Create the posterior 
~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 204-207

.. code-block:: Python


    z2 = cuqi.distribution.JointDistribution(theta,y2)(y2=data2)








.. GENERATED FROM PYTHON SOURCE LINES 208-209

We print the joint distribution `z2`:

.. GENERATED FROM PYTHON SOURCE LINES 209-211

.. code-block:: Python

    print(z2)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Posterior(
        Equation:
             p(theta|y2) ‚àù L(theta|y2)p(theta)
        Densities:
            y2 ~ CUQI Gaussian Likelihood function. Parameters ['theta'].
            theta ~ CUQI Gaussian.
     )




.. GENERATED FROM PYTHON SOURCE LINES 212-216

We see that we obtain a :class:`cuqi.distribution.Posterior` object, which
represents the posterior distribution of the parameters `theta` given the data
`y2`. The posterior distribution in this case is proportional to the product
of the likelihood obtained from the second data distribution and the prior.

.. GENERATED FROM PYTHON SOURCE LINES 218-220

Sample from the posterior
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 220-229

.. code-block:: Python


    # Sample from the posterior
    sampler = cuqi.legacy.sampler.MH(z2)
    samples = sampler.sample_adapt(8000)

    # Plot the credible interval and compute the ESS
    samples.burnthin(1000).plot_ci(95, exact=problemInfo1.exactSolution)
    samples.compute_ess()




.. image-sg:: /user/_auto_tutorials/images/sphx_glr_multiple_likelihoods_004.png
   :alt: multiple likelihoods
   :srcset: /user/_auto_tutorials/images/sphx_glr_multiple_likelihoods_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/CUQIpy/CUQIpy/cuqi/legacy/sampler/_sampler.py:156: UserWarning: 
    You are using the legacy sampler 'MH'.
    This will be removed in a future release of CUQIpy.
    Please consider using the new samplers in the 'cuqi.sampler' module.

      super().__init__(target, x0=x0, dim=dim, **kwargs)
    Sample 80 / 8000    Sample 160 / 8000    Sample 240 / 8000    Sample 320 / 8000    Sample 400 / 8000    Sample 480 / 8000    Sample 560 / 8000    Sample 640 / 8000    Sample 720 / 8000    Sample 800 / 8000    Sample 880 / 8000    Sample 960 / 8000    Sample 1040 / 8000    Sample 1120 / 8000    Sample 1200 / 8000    Sample 1280 / 8000    Sample 1360 / 8000    Sample 1440 / 8000    Sample 1520 / 8000    Sample 1600 / 8000    Sample 1680 / 8000    Sample 1760 / 8000    Sample 1840 / 8000    Sample 1920 / 8000    Sample 2000 / 8000    Sample 2080 / 8000    Sample 2160 / 8000    Sample 2240 / 8000    Sample 2320 / 8000    Sample 2400 / 8000    Sample 2480 / 8000    Sample 2560 / 8000    Sample 2640 / 8000    Sample 2720 / 8000    Sample 2800 / 8000    Sample 2880 / 8000    Sample 2960 / 8000    Sample 3040 / 8000    Sample 3120 / 8000    Sample 3200 / 8000    Sample 3280 / 8000    Sample 3360 / 8000    Sample 3440 / 8000    Sample 3520 / 8000    Sample 3600 / 8000    Sample 3680 / 8000    Sample 3760 / 8000    Sample 3840 / 8000    Sample 3920 / 8000    Sample 4000 / 8000    Sample 4080 / 8000    Sample 4160 / 8000    Sample 4240 / 8000    Sample 4320 / 8000    Sample 4400 / 8000    Sample 4480 / 8000    Sample 4560 / 8000    Sample 4640 / 8000    Sample 4720 / 8000    Sample 4800 / 8000    Sample 4880 / 8000    Sample 4960 / 8000    Sample 5040 / 8000    Sample 5120 / 8000    Sample 5200 / 8000    Sample 5280 / 8000    Sample 5360 / 8000    Sample 5440 / 8000    Sample 5520 / 8000    Sample 5600 / 8000    Sample 5680 / 8000    Sample 5760 / 8000    Sample 5840 / 8000    Sample 5920 / 8000    Sample 6000 / 8000    Sample 6080 / 8000    Sample 6160 / 8000    Sample 6240 / 8000    Sample 6320 / 8000    Sample 6400 / 8000    Sample 6480 / 8000    Sample 6560 / 8000    Sample 6640 / 8000    Sample 6720 / 8000    Sample 6800 / 8000    Sample 6880 / 8000    Sample 6960 / 8000    Sample 7040 / 8000    Sample 7120 / 8000    Sample 7200 / 8000    Sample 7280 / 8000    Sample 7360 / 8000    Sample 7440 / 8000    Sample 7520 / 8000    Sample 7600 / 8000    Sample 7680 / 8000    Sample 7760 / 8000    Sample 7840 / 8000    Sample 7920 / 8000    Sample 8000 / 8000    Sample 8000 / 8000

    Average acceptance rate: 0.146 MCMC scale: 0.0650357574625219 


    array([27.69442058, 24.71006971])



.. GENERATED FROM PYTHON SOURCE LINES 230-233

Formulate the Bayesian inverse problem using both data distributions (multiple likelihoods)
-------------------------------------------------------------------------------------------
We then formulate the Bayesian inverse problem using both data distributions and analyze the posterior samples.

.. GENERATED FROM PYTHON SOURCE LINES 235-237

Create the posterior 
~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 237-241

.. code-block:: Python


    z_joint = cuqi.distribution.JointDistribution(theta,y1,y2)(y1=data1, y2=data2)









.. GENERATED FROM PYTHON SOURCE LINES 242-243

We print the joint distribution `z_joint`:

.. GENERATED FROM PYTHON SOURCE LINES 243-245

.. code-block:: Python

    print(z_joint)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    MultipleLikelihoodPosterior(
        Equation: 
            p(theta|y1,y2) ‚àù p(theta)L(theta|y1)L(theta|y2)
        Densities: 
            theta ~ CUQI Gaussian.
            y1 ~ CUQI Gaussian Likelihood function. Parameters ['theta'].
            y2 ~ CUQI Gaussian Likelihood function. Parameters ['theta'].
     )




.. GENERATED FROM PYTHON SOURCE LINES 246-250

We see that in this case we obtain a :class:`MultipleLikelihoodPosterior` 
object, which represents the posterior distribution of the parameters `theta`
given the data `y1` and `y2`. The posterior distribution in this case is 
proportional to the product of the two likelihoods and the prior.

.. GENERATED FROM PYTHON SOURCE LINES 252-254

Sample from the posterior 
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 254-263

.. code-block:: Python


    # Sample from the posterior
    sampler = cuqi.legacy.sampler.MH(z_joint)
    samples = sampler.sample_adapt(8000)

    # Plot the credible interval and compute the ESS
    samples.burnthin(1000).plot_ci(95, exact=problemInfo1.exactSolution)
    samples.compute_ess()




.. image-sg:: /user/_auto_tutorials/images/sphx_glr_multiple_likelihoods_005.png
   :alt: multiple likelihoods
   :srcset: /user/_auto_tutorials/images/sphx_glr_multiple_likelihoods_005.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/CUQIpy/CUQIpy/cuqi/legacy/sampler/_sampler.py:156: UserWarning: 
    You are using the legacy sampler 'MH'.
    This will be removed in a future release of CUQIpy.
    Please consider using the new samplers in the 'cuqi.sampler' module.

      super().__init__(target, x0=x0, dim=dim, **kwargs)
    Sample 80 / 8000    Sample 160 / 8000    Sample 240 / 8000    Sample 320 / 8000    Sample 400 / 8000    Sample 480 / 8000    Sample 560 / 8000    Sample 640 / 8000    Sample 720 / 8000    Sample 800 / 8000    Sample 880 / 8000    Sample 960 / 8000    Sample 1040 / 8000    Sample 1120 / 8000    Sample 1200 / 8000    Sample 1280 / 8000    Sample 1360 / 8000    Sample 1440 / 8000    Sample 1520 / 8000    Sample 1600 / 8000    Sample 1680 / 8000    Sample 1760 / 8000    Sample 1840 / 8000    Sample 1920 / 8000    Sample 2000 / 8000    Sample 2080 / 8000    Sample 2160 / 8000    Sample 2240 / 8000    Sample 2320 / 8000    Sample 2400 / 8000    Sample 2480 / 8000    Sample 2560 / 8000    Sample 2640 / 8000    Sample 2720 / 8000    Sample 2800 / 8000    Sample 2880 / 8000    Sample 2960 / 8000    Sample 3040 / 8000    Sample 3120 / 8000    Sample 3200 / 8000    Sample 3280 / 8000    Sample 3360 / 8000    Sample 3440 / 8000    Sample 3520 / 8000    Sample 3600 / 8000    Sample 3680 / 8000    Sample 3760 / 8000    Sample 3840 / 8000    Sample 3920 / 8000    Sample 4000 / 8000    Sample 4080 / 8000    Sample 4160 / 8000    Sample 4240 / 8000    Sample 4320 / 8000    Sample 4400 / 8000    Sample 4480 / 8000    Sample 4560 / 8000    Sample 4640 / 8000    Sample 4720 / 8000    Sample 4800 / 8000    Sample 4880 / 8000    Sample 4960 / 8000    Sample 5040 / 8000    Sample 5120 / 8000    Sample 5200 / 8000    Sample 5280 / 8000    Sample 5360 / 8000    Sample 5440 / 8000    Sample 5520 / 8000    Sample 5600 / 8000    Sample 5680 / 8000    Sample 5760 / 8000    Sample 5840 / 8000    Sample 5920 / 8000    Sample 6000 / 8000    Sample 6080 / 8000    Sample 6160 / 8000    Sample 6240 / 8000    Sample 6320 / 8000    Sample 6400 / 8000    Sample 6480 / 8000    Sample 6560 / 8000    Sample 6640 / 8000    Sample 6720 / 8000    Sample 6800 / 8000    Sample 6880 / 8000    Sample 6960 / 8000    Sample 7040 / 8000    Sample 7120 / 8000    Sample 7200 / 8000    Sample 7280 / 8000    Sample 7360 / 8000    Sample 7440 / 8000    Sample 7520 / 8000    Sample 7600 / 8000    Sample 7680 / 8000    Sample 7760 / 8000    Sample 7840 / 8000    Sample 7920 / 8000    Sample 8000 / 8000    Sample 8000 / 8000

    Average acceptance rate: 0.067375 MCMC scale: 0.04423692692875009 


    array([225.17033864, 219.04385327])



.. GENERATED FROM PYTHON SOURCE LINES 264-270

We notice that combining the two data distributions leads to a more certain 
estimate of the conductivity (using the same number of MCMC iterations).
This is because including the two different data sets in the inversion is more
informative than the single data set case. Also, the effective sample size is
larger than (or comparable to) what is obtained in any of the single data
distribution case.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 17.970 seconds)


.. _sphx_glr_download_user__auto_tutorials_multiple_likelihoods.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: multiple_likelihoods.ipynb <multiple_likelihoods.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: multiple_likelihoods.py <multiple_likelihoods.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: multiple_likelihoods.zip <multiple_likelihoods.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
