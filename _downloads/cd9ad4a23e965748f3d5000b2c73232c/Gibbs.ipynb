{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Gibbs sampling\n\n    This tutorial shows how to use CUQIpy to perform Gibbs sampling.\n    Gibbs sampling is a Markov chain Monte Carlo (MCMC) method for\n    sampling a joint probability distribution.\n\n    Opposed to jointly sampling the distribution simultaneously, Gibbs\n    sampling samples the variables of the distribution sequentially,\n    one variable at a time. When a variable represents a random vector, the\n    whole vector is sampled simultaneously.\n    \n    The sampling of each variable is done by sampling from the conditional\n    distribution of that variable given (fixed, previously sampled) values\n    of the other variables.\n\n    This is often a very efficient way of sampling from a joint\n    distribution if the conditional distributions are easy to sample\n    from. This is one way to exploit the structure of the joint\n    distribution. On the other hand, if the conditional distributions\n    are highly correlated and/or are difficult to sample from, then\n    Gibbs sampling can be very inefficient.\n\n    For these reasons, Gibbs sampling is often a double-edged sword,\n    that needs to be used in the right context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\nWe start by importing the necessary modules\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom cuqi.testproblem import Deconvolution1D\nfrom cuqi.distribution import Gaussian, Gamma, JointDistribution, GMRF, LMRF\nfrom cuqi.legacy.sampler import Gibbs, LinearRTO, Conjugate, UGLA, ConjugateApprox\n\nnp.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forward model and data\nWe define the forward model and data.\nHere we use a 1D deconvolution problem, so the forward model is linear,\nthat is:\n\n\\begin{align}\\mathbf{y} = \\mathbf{A} \\mathbf{x}\\end{align}\n\nwhere $\\mathbf{A}$ is the convolution matrix, and $\\mathbf{x}$ is the input signal.\n\nWe load this example from the testproblem library of CUQIpy and visualize the\ntrue solution (sharp signal) and data (convolved signal).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Model and data\nA, y_obs, probinfo = Deconvolution1D(phantom='square').get_components()\n\n# Get dimension of signal\nn = A.domain_dim\n\n# Plot exact solution and observed data\nplt.subplot(121)\nprobinfo.exactSolution.plot()\nplt.title('exact solution')\n\nplt.subplot(122)\ny_obs.plot()\nplt.title(\"Observed data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hierarchical Bayesian model\n\nWe define the following hierarchical model:\n\n\\begin{align}\\begin{align}\n        d &\\sim \\mathrm{Gamma}(1, 10^{-4}) \\\\\n        l &\\sim \\mathrm{Gamma}(1, 10^{-4}) \\\\\n        \\mathbf{x} &\\sim \\mathrm{GMRF}(\\mathbf{0}, d) \\\\\n        \\mathbf{y} &\\sim \\mathcal{N}(\\mathbf{A} \\mathbf{x}, l^{-1} \\mathbf{I}_m)\n    \\end{align}\\end{align}\n\nwhere $\\mathbf{y}$ is the observed data, and $\\mathbf{x}$\nis the unknown signal. The hyperparameters $d$ and $l$ are\nthe precision of the prior distribution of $\\mathbf{x}$ and\nthe noise, respectively.\n\nThe prior distribution of $\\mathbf{x}$ is a Gaussian Markov random\nfield (GMRF) with zero mean and precision $d$. It can\nbe viewed as a Gaussian prior on the differences between neighboring\nelements of $\\mathbf{x}$.\n\nIn CUQIpy the model can be defined as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define distributions\nd = Gamma(1, 1e-4)\nl = Gamma(1, 1e-4)\nx = GMRF(np.zeros(n), lambda d: d)\ny = Gaussian(A, lambda l: 1/l)\n\n# Combine into a joint distribution\njoint = JointDistribution(d, l, x, y)\n\n# View the joint distribution\nprint(joint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the joint distribution prints a mathematical expression\nfor the density functions that make up $p(d,l,\\mathbf{x},\\mathbf{y})$.\nIn this case they are all distributions, but this need not be the case.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the posterior distribution\n\nNow we define the posterior distribution, which is the joint distribution\nconditioned on the observed data. That is, $p(d, l, \\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}_\\mathrm{obs})$\n\nThis is done in the following way:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define posterior by conditioning on the data\nposterior = joint(y=y_obs)\n\n# View the structure of the posterior\nprint(posterior)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that after conditioning on the data, the distribution associated with\n$\\mathbf{y}$ became a likelihood function and that the posterior is now\na joint distribution of the variables $d$, $l$, $\\mathbf{x}$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gibbs Sampler\n\nThe hierarchical model above has some important properties that we\ncan exploit to make the sampling more efficient. First, note that\nthe Gamma distribution are conjugate priors for the precision of\nthe Gaussian distributions. This means that we can efficiently sample\nfrom $d$ and $l$ conditional on the other variables.\n\nSecond, note that the prior distribution of $\\mathbf{x}$ is\na Gaussian Markov random field (GMRF) and that the distribution for\n$\\mathbf{y}$ is also Gaussian with a Linear operator acting\non $\\mathbf{x}$ as the mean variable. This means that we can\nefficiently sample from $\\mathbf{x}$ conditional on the other\nvariables using the ``LinearRTO`` sampler.\n\nTaking these two facts into account, we can define a Gibbs sampler\nthat uses the ``Conjugate`` sampler for $d$ and $l$ and\nthe ``LinearRTO`` sampler for $\\mathbf{x}$.\n\nThis is done in CUQIpy as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define sampling strategy\nsampling_strategy = {\n    'x': LinearRTO,\n    'd': Conjugate,\n    'l': Conjugate\n}\n\n# Define Gibbs sampler\nsampler = Gibbs(posterior, sampling_strategy)\n\n# Run sampler\nsamples = sampler.sample(Ns=1000, Nb=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze results\n\nAfter sampling we can inspect the results. The samples are stored\nas a dictionary with the variable names as keys. Samples for each \nvariable is stored as a CUQIpy Samples object which contains the\nmany convenience methods for diagnostics and plotting of MCMC samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot credible intervals for the signal\nsamples['x'].plot_ci(exact=probinfo.exactSolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trace plot for d\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samples['d'].plot_trace(figsize=(8,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trace plot for l\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samples['l'].plot_trace(figsize=(8,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Switching to a piecewise constant prior\n\nNotice that while the sampling went well in the previous example,\nthe posterior distribution did not match the characteristics of\nthe exact solution. We can improve this result by switching to a\nprior that better matches the exact solution $\\mathbf{x}$.\n\nOne choice is the Laplace difference prior, which assumes a\nLaplace distribution for the differences between neighboring\nelements of $\\mathbf{x}$. That is,\n\n\\begin{align}\\mathbf{x} \\sim \\text{LMRF}(d^{-1}),\\end{align}\n\nwhich means that $x_i-x_{i-1} \\sim \\mathrm{Laplace}(0, d^{-1})$.\n\nThis prior is implemented in CUQIpy as the ``LMRF`` distribution.\nTo update our model we simply need to replace the ``GMRF`` distribution\nwith the ``LMRF`` distribution. Note that the Laplace distribution\nis defined via a scale parameter, so we invert the parameter $d$.\n\nThis laplace distribution and new posterior can be defined as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define new distribution for x\nx = LMRF(0, lambda d: 1/d, geometry=n)\n\n# Define new joint distribution with piecewise constant prior\njoint_Ld = JointDistribution(d, l, x, y)\n\n# Define new posterior by conditioning on the data\nposterior_Ld = joint_Ld(y=y_obs)\n\nprint(posterior_Ld)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gibbs Sampler (with Laplace prior)\n\nUsing the same approach as earlier we can define a Gibbs sampler\nfor this new hierarchical model. The only difference is that we\nnow need to use a different sampler for $\\mathbf{x}$ because\nthe ``LinearRTO`` sampler only works for Gaussian distributions.\n\nIn this case we use the UGLA (Unadjusted Gaussian Laplace Approximation) sampler\nfor $\\mathbf{x}$. We also use an approximate Conjugate\nsampler for $d$ which approximately samples from the\nposterior distribution of $d$ conditional on the other\nvariables in an efficient manner. For more details see e.g.\n`this paper <https://arxiv.org/abs/2104.06919>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define sampling strategy\nsampling_strategy = {\n    'x': UGLA,\n    'd': ConjugateApprox,\n    'l': Conjugate\n}\n\n# Define Gibbs sampler\nsampler_Ld = Gibbs(posterior_Ld, sampling_strategy)\n\n# Run sampler\nsamples_Ld = sampler_Ld.sample(Ns=1000, Nb=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze results\n\nAgain we can inspect the results.\nHere we notice the posterior distribution matches the exact solution much better.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot credible intervals for the signal\nsamples_Ld['x'].plot_ci(exact=probinfo.exactSolution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samples_Ld['d'].plot_trace(figsize=(8,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samples_Ld['l'].plot_trace(figsize=(8,2))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}