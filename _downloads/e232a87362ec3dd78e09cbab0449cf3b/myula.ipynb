{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How to sample with MYULA\n\nThe recommended way to define a posterior distribution in CUQIpy is to use the\n:class:`~cuqi.distribution.JointDistribution` class to define the joint\ndistribution of the parameters and the data and then condition on observed\ndata to obtain the posterior distribution as shown in the examples below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import cuqi\nimport numpy as np\nfrom cuqi.implicitprior import RestorationPrior, MoreauYoshidaPrior\nfrom cuqi.sampler import ULA, MYULA\nfrom cuqi.distribution import Posterior\nimport matplotlib.pyplot as plt\n\nfrom skimage.metrics import normalized_root_mse as nrmse\nfrom skimage.metrics import mean_squared_error as mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A simple Bayesian inverse problem\n\nConsider a deconvolution inverse problem given by\n\n\\begin{align}\\mathbf{y} = \\mathbf{A}\\mathbf{x}.\\end{align}\n\nSee :class:`~cuqi.testproblem.Deconvolution1D` for more details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "A, y_obs, info = cuqi.testproblem.Deconvolution1D().get_components()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Principles behind MYULA\nThe goal is to solve this inverse problem by sampling from the posterior\ndistribution given by $\\pi(x|y) \\propto \\pi(x) \\pi(y|x)$.\nWe assume a Gaussian likelihood, ie $- \\log \\pi(y|x) = \\|Ax-y \\|_2^2/2 \\texttt{sigma2}$\nand a prior such that $- \\log \\pi (x) =  g(x)$ with $g$ convex.\nTo sample from $\\pi(x|y)$, we are going to apply a ULA based algorithm,\nMYULA (https://arxiv.org/pdf/1612.07471).\nWe recall that ULA\n\n\\begin{align}x_{k+1} = x_k + \\texttt{scale} \\nabla \\log \\pi(x_k |y) + \\sqrt{2 \\texttt{scale}} z_{k+1}\\end{align}\n\n\\begin{align}x_{k+1} = x_k + \\texttt{scale} \\nabla \\log \\pi(y | x_k) + \\texttt{scale} \\nabla \\log \\pi(x_k) + \\sqrt{2 \\texttt{scale}} z_{k+1}\\end{align}\n\nwith $(z_k)_{k \\in \\mathbb{N}^*}$ a sequence of independent and\nidentically distributed Gaussian random variables\nwith zero mean and identity covariance.\n\nIn the case where $\\log \\pi(x)$ is not differentiable we can\nunfortunately not apply ULA. The idea is to consider a surrogate\nposterior density $\\pi_{\\texttt{smoothing_strength}} (x|y) \\propto \\pi(y|x) \\pi_{\\texttt{smoothing_strength}} (x)$\nwhere\n\n\\begin{align}\\pi_{\\texttt{smoothing_strength}}(x) \\propto \\exp(- g_{\\texttt{smoothing_strength}} (x))\\end{align}\n\nand $g_{\\texttt{smoothing_strength}}$ is the\n$\\texttt{smoothing_strength}$-Moreau envelope of $g$, ie\n\n\\begin{align}g_\\texttt{smoothing_strength}(x) = \\operatorname{inf}_z \\| x- z \\|_2^2/2\\texttt{smoothing_strength} + g(z).\\end{align}\n\n$g_{\\texttt{smoothing_strength}}$ is continuously differentiable with $1/\\texttt{smoothing_strength}$-Lipschitz gradient and s.t\n\n\\begin{align}\\nabla g_{\\texttt{smoothing_strength}} (x) = (x- \\operatorname{prox}_g^{\\texttt{smoothing_strength}} (x))/\\texttt{smoothing_strength}\\end{align}\n\nwith\n\n\\begin{align}\\operatorname{prox}_g^{\\texttt{smoothing_strength}} (x) = \\operatorname{argmin}_z \\|x-z \\|_2^2/2\\texttt{smoothing_strength} + g(z)\\end{align}\n\nSee https://link.springer.com/chapter/10.1007/978-3-319-48311-5_31 for more details.\n\nMYULA consists in applying ULA to a smoothed target distribution. It reads\n\n\\begin{align}\\begin{align*}\n      x_{k+1} &= x_k + \\texttt{scale} \\nabla \\log \\pi_{\\texttt{smoothing_strength}}(x_k |y) + \\sqrt{2 \\texttt{scale}} z_{k+1}\\\\\n      &= x_k + \\texttt{scale} \\nabla \\log \\pi(y | x_k) + \\texttt{scale} \\nabla \\log \\pi_{\\texttt{smoothing_strength}}(x_k) + \\sqrt{2 \\texttt{scale}} z_{k+1}\\\\\n      &= x_k + \\texttt{scale} \\nabla \\log \\pi(y | x_k) - \\texttt{scale} (x_k - \\operatorname{prox}_g^{\\texttt{smoothing_strength}} (x_k))/{\\texttt{smoothing_strength}} + \\sqrt{2 \\texttt{scale}} z_{k+1}.\n      \\end{align*}\\end{align}\n\nwhere $\\texttt{smoothing_strength}$ corresponds to the smoothing strength of $g$.\n\nTo illustrate MYULA, we will consider $g(x) = \\texttt{regularization_strength} \\  TV(x) = \\texttt{regularization_strength} \\|\\nabla x \\|_{2, 1}$,\nwhere $\\texttt{regularization_strength}$ is the regularization parameter which\ncontrols the regularization strength induced by TV.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian model definition\nThen consider the following Bayesian Inverse Problem:\n\n\\begin{align}\\begin{align*}\n   \\mathbf{x} &\\sim \\exp (- \\texttt{regularization_strength} \\|\\nabla x \\|_{2,1})\\\\\n   \\mathbf{y}_{obs} &\\sim \\mathcal{N}(\\mathbf{A}\\mathbf{x}, \\texttt{sigma2}\\,\\mathbf{I}) \\ ,\n   \\end{align*}\\end{align}\n\nwith $\\texttt{sigma2}=0.05^2$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Likelihood definition\nWe first specify the data distribution as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma2 = 0.05**2\ny = cuqi.distribution.Gaussian(A, sigma2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we can define the likelihood with\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "likelihood = y(y=y_obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RestorationPrior and MoreauYoshidaPrior\nTo apply MYULA, we need to define the Moreau-Yoshida prior\n$\\pi_{\\texttt{smoothing_stength}}(x)$.\nEvaluating this surrogate prior is doable but too intensive from\na computational point of view as it requires to solve an optimization problem.\nHowever to apply MYULA, we only require access to\n$\\operatorname{prox}_{\\texttt{regularization_strength}\\ TV}^{\\texttt{smoothing_strength}}$.\n$\\operatorname{prox}_{\\texttt{regularization_strength}\\ TV}^{\\texttt{smoothing_strength}}$\nis a denoising operator (also called denoiser), which takes a signal as input\nand returns a less noisy\nsignal. In CUQIPy, we talk about restoration operators (also called restorators).\nDenoisers are an example of restorators.\nRestorators are at the\ncore of a specific type of priors called RestorationPrior. We cannot sample\nfrom these priors but they allow us to define other types of priors.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RestorationPrior definition\nA restorator, is associated with a parameter called\n$\\texttt{restoration_strength}$. This parameter indicates how strong is\nthe restoration. For example, when this restorator is a denoiser, an operator\ntaking an signal as input and returning a less noisy signal, $\\texttt{restoration_strength}$\ncan correspond to the  denoising level.\nIn the following, we consider the denoiser\n$\\operatorname{prox}_{\\texttt{regularization_strength}\\ TV}^{\\texttt{restoration_strength}}$.\nWe use the implementation provided by Scikit-Image. But we can use any solver\nto compute this quantity.\nWe emphasize that we have for any $g$\n\n\\begin{align}\\operatorname{prox}_{\\texttt{regularization_strength}\\  g}^{\\texttt{smoothing_strength}} = \\operatorname{prox}_{g}^{\\texttt{weight}} ,\\end{align}\n\nwith $\\texttt{weight} = \\texttt{regularization_strength} \\times  \\texttt{smoothing_strength}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "regularization_strength = 10\nrestoration_strength = 0.5 * sigma2\nfrom skimage.restoration import denoise_tv_chambolle\n\n\ndef prox_g(x, regularization_strength=None, restoration_strength=None):\n    weight = regularization_strength * restoration_strength\n    return denoise_tv_chambolle(x, weight=weight, max_num_iter=100), None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We save all the important variables into the variable\n$\\texttt{restorator_kwargs}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "restorator_kwargs = {}\nrestorator_kwargs[\"regularization_strength\"] = regularization_strength"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can define our RestorationPrior.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "restorator = RestorationPrior(\n    prox_g,\n    restorator_kwargs=restorator_kwargs,\n    geometry=likelihood.model.domain_geometry,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first apply the restorate method of our denoiser to $\\mathbf{y}_{obs}$.\nThis operator should restore $\\mathbf{y}_{obs}$ and generate a signal close\nto $\\mathbf{A}\\mathbf{x}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "res = restorator.restore(y_obs, restoration_strength)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this cell, we show the effect of the restorator both from a visual\nand quantitative point of view. We use the relative error and the mean-squared\nerror. The smaller are these quantities, the better it is.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\ny_obs.plot(\n    label=\"observation (Relative error={:.5f})\".format(nrmse(info.exactData, y_obs))\n)\nres.plot(label=\"restoration (Relative error={:.5f})\".format(nrmse(info.exactData, res)))\ninfo.exactData.plot(label=\"groundtruth\")\nplt.legend()\n\nprint(\n    \"MSE(Ax, y_obs) is \",\n    mse(info.exactData, y_obs) / mse(info.exactData, res),\n    \" times larger than MSE(Ax, res).\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definition of the Moreau-Yoshida prior\nIt is a smoothed version from the target prior. Its definition requires a prior\nof type RestorationPrior and a scalar parameter $\\texttt{smoothing_strength}$\nwhich controls the strength of the smoothing. We must have\n$\\texttt{smoothing_strength}=\\texttt{restoration_strength}$.\n\nAs suggested by Durmus et al. (https://arxiv.org/pdf/1612.07471), we set the\nsmoothing parameter $\\texttt{smoothing_strength} \\approx \\texttt{sigma2}$,\nie $\\texttt{smoothing_strength}= 0.5 \\ \\texttt{sigma2}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "myprior = MoreauYoshidaPrior(prior=restorator, smoothing_strength=restoration_strength)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implicitly defined posterior distribution\nWe can now define the implicitly defined smoothed posterior distribution as\nfollows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "smoothed_posterior = Posterior(likelihood, myprior)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters of the MYULA sampler\nWe let run MYULA for $\\texttt{Ns}=10^4$\niterations. We discard the $\\texttt{Nb}=1000$ first burn-in samples of\nthe Markov chain. Furthermore, as MCMC methods generate\ncorrelated samples, we also perform a thinning: we only consider 1 samples\nevery $\\texttt{Nt}=20$\nsamples to compute our quantities of interest.\n$\\texttt{scale}$ is set wrt the recommendation of Durmus et al.\n(https://arxiv.org/pdf/1612.07471). It must be smaller than the inverse of the\nLipschitz constant of the gradient of the log-posterior density. In this setting,\nThe Lipschitz constant of the gradient of likelihood log-density is\n$\\|A^TA \\|_2^2/\\texttt{sigma2}$ and the one of the log-prior is\n$1/\\texttt{smoothing_strength}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Ns = 10000\nNb = 1000\nNt = 20\n# Step-size of MYULA\nscale = 0.9 / (1 / sigma2 + 1 / restoration_strength)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to get reproducible results, we set the seed parameter to 0.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ULA sampler\nDefinition of the ULA sampler which aims at sampling from the smoothed posterior.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ula_sampler = ULA(target=smoothed_posterior, scale=scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sampling with ULA from the smoothed target posterior.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ula_sampler.sample(Ns=Ns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieve the samples. We apply the burnin and perform thinning to the Markov\nchain.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samples = ula_sampler.get_samples()\nsamples_warm = samples.burnthin(Nb=Nb, Nt=Nt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\nMean and CI plot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\ny_obs.plot(label=\"Observation\")\nsamples_warm.plot_ci(exact=info.exactSolution)\nplt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Standard  deviation plot to estimate the uncertainty.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\nsamples_warm.plot_std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Samples autocorrelation plot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samples_warm.plot_autocorrelation(max_lag=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Other way to sample with MYULA\nTo sample with MYULA, we can also define an implicit posterior with a\nRestorationPrior object (instead of MoreauYoshidaPrior object) and then automatically\nperform the Moreau-Yoshida smoothing when defining\nthe MYULA sampler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "posterior = Posterior(likelihood, restorator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definition of the MYULA sampler\nAgain, we must have $\\texttt{smoothing_strength}=\\texttt{restoration_strength}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "myula_sampler = MYULA(\n    target=posterior, scale=scale, smoothing_strength=restoration_strength\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then sample using the MYULA sampler. It targets the same smoothed distribution\nas the ULA sampler applied with the smoothed posterior distribution.\nIf the samples generated by ULA and MYULA are the same, then a message \"MYULA\nsamples of the posterior are the same ULA samples from the smoothed\nposterior\"\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\nmyula_sampler.sample(Ns=Ns)\nsamples_myula = myula_sampler.get_samples()\nsamples_myula_warm = samples_myula.burnthin(Nb=Nb, Nt=Nt)\nassert np.allclose(samples_warm.samples, samples_myula_warm.samples)\nprint(\n    \"MYULA samples of the posterior are the same ULA samples from the smoothed \\\nposterior\"\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}